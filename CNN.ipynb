{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка качества классификатора**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "file='articles.csv'\n",
    "\n",
    "with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    for row in reader:            \n",
    "        if row!=[]:\n",
    "            kws=''.join(row[0])\n",
    "            text = ''.join(row[2])\n",
    "            sentences_origin = text.split('.')\n",
    "            sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "            kws_tr = treatment_text(kws)  \n",
    "            kws_tr_set=set(kws_tr)  \n",
    "            for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                sentence_tr_set=set(sentence_tr)\n",
    "                if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                    dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                elif len(sentence_tr)>5:\n",
    "                    dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cnn_model.pickle', 'rb') as f:\n",
    "    cls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка качества выделения КС**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from deeppavlov.metrics.accuracy import sets_accuracy\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# credentials = service_account.Credentials.from_service_account_file('./diplom-275218-a097a7baee3f.json')\n",
    "# project_id = 'diplom-275218'\n",
    "# private_key = 'diplom-275218-a097a7baee3f.json'\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "\n",
    "dict_stop=set(['метод','определение','условие','момент','значение','результат','критерий',\n",
    "               'работа','вариант','брянский государственный университет','научнотехнический вестник',\n",
    "              'соответствие','такой образ','весь критерий','пример','выбор','ключевое слово','период',\n",
    "              'уравнение','формула','множитель','повышение','оценка','проведение',\n",
    "              'машина','нагрузка','брянская область','точка','случай','расчет','таблица','расчёт',\n",
    "              'с показатель','град','обработка','статья','элемент','раз','применение','центр','форма','важная задача'])\n",
    "\n",
    "\n",
    "with open('./cls_model.pickle', 'rb') as f:\n",
    "    cls = pickle.load(f)\n",
    "\n",
    "with open('./tf_idf_model.pickle', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "def treatment_text2(text):\n",
    "    text = re.sub(\"[^а-яА-Яa-zA-Z0-9,.?]\", \" \", str(text))\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    while text.find('  ')!=-1:\n",
    "        text = text.replace('  ',' ')\n",
    "    text = str(text)\n",
    "    return text\n",
    "\n",
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-ЯЁёa-zA-Z0-9]\", \" \", review)\n",
    "    review_text = review_text.replace('ё','е')\n",
    "    review_text = review_text.replace('Ё', 'Е')\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "\n",
    "def normalize(keyphrase):\n",
    "    try:\n",
    "        words = keyphrase.split()\n",
    "        if len(words)>1:\n",
    "            main_noun = words[-1]\n",
    "            flag_noun = False\n",
    "            flag_adj = False\n",
    "            for i in words[:-1]:\n",
    "                if 'NOUN' not in  morph.parse(i)[0].tag and 'ADJF' not in morph.parse(i)[0].tag:\n",
    "                    return morph.parse(words[0])[0].lexeme[0][0] + ','+morph.parse(words[2])[0].lexeme[0][0]\n",
    "                if 'NOUN' in morph.parse(i)[0].tag:\n",
    "                    flag_noun=True\n",
    "                if 'ADJF' in morph.parse(i)[0].tag:\n",
    "                    flag_adj=True\n",
    "            s = ''\n",
    "\n",
    "            if flag_noun is False:\n",
    "                for word in words[:-1]:\n",
    "                    for i in morph.parse(word)[0].lexeme:\n",
    "                        if morph.parse(main_noun)[0].tag.gender in i.tag and 'nomn' in i.tag:\n",
    "\n",
    "                            s+=i.word + ' '\n",
    "                            break\n",
    "                return s + morph.parse(main_noun)[0].lexeme[0][0]\n",
    "\n",
    "            elif flag_noun is True and flag_adj is False:\n",
    "                s = morph.parse(words[0])[0].lexeme[0][0]\n",
    "                for i in words[1:]:\n",
    "                    try:\n",
    "                        s+= ' '+morph.parse(i)[0].lexeme[1][0]\n",
    "                    except:\n",
    "                        pass\n",
    "                return s\n",
    "\n",
    "            else:\n",
    "                flag_first_noun=False\n",
    "\n",
    "                for word in words:\n",
    "                    if 'ADJF' in  morph.parse(word)[0].tag and flag_first_noun is False:\n",
    "                        rod = find_rod(words[words.index(word)+1:])\n",
    "                        for i in morph.parse(word)[0].lexeme:\n",
    "                            if rod in i.tag and 'nomn' in i.tag:\n",
    "\n",
    "                                s+=i.word + ' '\n",
    "                                break\n",
    "                    elif 'ADJF' in  morph.parse(word)[0].tag and flag_first_noun is True:\n",
    "                        rod = find_rod(words[words.index(word)+1:])\n",
    "                        for i in morph.parse(word)[0].lexeme:\n",
    "                            if rod in i.tag and 'gent' in i.tag:\n",
    "                                s+=i.word + ' '\n",
    "                                break\n",
    "                    elif  'NOUN' in  morph.parse(word)[0].tag and flag_first_noun is False:\n",
    "                        flag_first_noun=True\n",
    "\n",
    "                        s += morph.parse(word)[0].lexeme[0][0] + ' '\n",
    "                    else:\n",
    "\n",
    "                        s += morph.parse(word)[0].lexeme[2][0] +' '\n",
    "\n",
    "\n",
    "            return s\n",
    "        elif len(words)==1:\n",
    "            return morph.parse(words[0])[0].lexeme[0][0]\n",
    "    except:\n",
    "        pass\n",
    "#         print(' '.join(words),'НЕ МОЖЕТ БЫТЬ ПРЕОБРАЗОВАН')\n",
    "\n",
    "def find_rod(words):\n",
    "    for w in words:\n",
    "        if 'NOUN' in morph.parse(w)[0].tag:\n",
    "            return morph.parse(w)[0].tag.gender\n",
    "\n",
    "\n",
    "def find_adj(sentence, index_noun, pred_noun):\n",
    "    if pred_noun == 0 and index_noun == 0:\n",
    "        return 0\n",
    "    if pred_noun == 0 and index_noun != 0:\n",
    "        i = index_noun - 1\n",
    "        flag_ad = False\n",
    "        flag_noun = False\n",
    "        while i >= 0:\n",
    "            if 'ADJF' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_ad = True\n",
    "            elif 'NOUN' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_noun = True\n",
    "            elif flag_ad is False and flag_noun is False:\n",
    "                return i + 1\n",
    "\n",
    "            flag_ad = False\n",
    "            flag_noun = False\n",
    "            i -= 1\n",
    "        return i + 1\n",
    "\n",
    "    else:\n",
    "        i = index_noun - 1\n",
    "        flag_ad = False\n",
    "        flag_noun = False\n",
    "        while i > pred_noun:\n",
    "            if 'ADJF' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_ad = True\n",
    "            elif 'NOUN' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_noun = True\n",
    "            elif flag_ad is False and flag_noun is False:\n",
    "                return i + 1\n",
    "\n",
    "            flag_ad = False\n",
    "            flag_noun = False\n",
    "            i -= 1\n",
    "        return i + 1\n",
    "\n",
    "\n",
    "def keyword_extraction(text):\n",
    "    sentences_origin = text.split('.')\n",
    "    sentences_tr = list(map(treatment_text, sentences_origin))\n",
    "    test=[]\n",
    "    for sentence_tr, sentence_or in zip(sentences_tr, sentences_origin):\n",
    "        # sentence_tr_set = set(sentence_tr)\n",
    "        # if len(sentence_tr) > 5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "        #     test.append((sentence_or, ' '.join(sentence_tr)))\n",
    "        if len(sentence_tr) > 5:\n",
    "            test.append((sentence_or, ' '.join(sentence_tr)))\n",
    "    x_test = []\n",
    "    for s in test:\n",
    "        x_test.append(s[1])\n",
    "#     x_test = tuple(x_test)\n",
    "#     y_pred = model.predict(tokenizer.texts_to_matrix(x_test, mode='binary'))\n",
    "#     print(y_pred,x_test)\n",
    "    x_test = [i.split(' ')[:20] for i in x_test]\n",
    "    vecX = vectorize(x_test, sentence_maxlen, w2i)\n",
    "    result =  model.predict(vecX)\n",
    "    \n",
    "    y_pred2 = []\n",
    "    for i in result:\n",
    "        if i[0]>=0.5:\n",
    "            y_pred2.append('1')\n",
    "        else:\n",
    "            y_pred2.append('0')\n",
    "    \n",
    "\n",
    "    sentences_with_kw = []\n",
    "    for i in range(len(y_pred2)):\n",
    "        if y_pred2[i] == '1':\n",
    "#             print(test[i])\n",
    "            sentences_with_kw.append(test[i])\n",
    "    dict_cand = {}\n",
    "    for sentence in sentences_with_kw:\n",
    "        cand = extract_candidate(sentence[0], sentences_with_kw)\n",
    "        cand = list(map(normalize, cand))\n",
    "        for key_cand in cand:\n",
    "            if key_cand in dict_cand:\n",
    "                dict_cand[key_cand] += 1\n",
    "            else:\n",
    "                dict_cand[key_cand] = 1\n",
    "    dict_cand_sort = {k: v for k, v in sorted(dict_cand.items(), key=lambda item: item[1], reverse=True)}\n",
    "    result = []\n",
    "    count = 0\n",
    "    k = set(['1', '2', '3', '4', '5', '&', '?', '/', '\\\\', '!'])\n",
    "\n",
    "    for word in dict_cand_sort.keys():\n",
    "        try:\n",
    "            if count < 10:\n",
    "                if ',' in word:\n",
    "                    word = word.split()\n",
    "                    result += word\n",
    "                elif k.intersection(set(word)) == set() and word not in dict_stop and 'ADJF' not in morph.parse(word)[\n",
    "                    0].tag:\n",
    "                    result.append(word)\n",
    "                    count += 1\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    for i in result:\n",
    "\n",
    "        flag = False\n",
    "        s = i.replace(',', ' ')\n",
    "        s = s.split(' ')\n",
    "        for j in s:\n",
    "            if 'NOUN' in morph.parse(j)[0].tag:\n",
    "                flag = True\n",
    "        if flag is False:\n",
    "            result.remove(i)\n",
    "    result = list(map(lambda x: x.replace(',', ' '), result))\n",
    "    result = ', '.join(result)\n",
    "    if result!='' :\n",
    "        return result\n",
    "#     else:\n",
    "#         keywords = r.keywords_extract(text)\n",
    "#         keywords = ', '.join(keywords).capitalize()\n",
    "#         return keywords\n",
    "def extract_candidate(origin, sentences_with_kw):\n",
    "    candidate = []\n",
    "    x = re.sub(\"[^а-яА-Я]\", \" \", origin)\n",
    "    while x.find('  ') != -1:\n",
    "        x = x.replace('  ', ' ')\n",
    "    x = x.split()\n",
    "    x = list(filter(lambda x: len(x) > 2, x))\n",
    "    pr = 0\n",
    "\n",
    "    for i in range(len(x) - 1):\n",
    "        if 'NOUN' in morph.parse(x[i])[0].tag and 'NOUN' in morph.parse(x[i + 1])[0].tag and i + 2 != len(x):\n",
    "            pass\n",
    "        elif 'NOUN' in morph.parse(x[i])[0].tag and 'NOUN' in morph.parse(x[i + 1])[0].tag and i + 2 == len(x):\n",
    "            try:\n",
    "                k = find_adj(x, i + 2, pr)\n",
    "                pr = i\n",
    "                if ' '.join(x[k:i + 1]) in sentences_with_kw[2][0]:\n",
    "                    candidate.append(' '.join(x[k:i + 2]))\n",
    "                else:\n",
    "                    begin = origin.find(x[k])\n",
    "                    end = origin.find(x[i + 1]) + len(x[i + 1])\n",
    "                    candidate += origin[begin:end].split(',')\n",
    "            except:\n",
    "                pass\n",
    "        elif 'NOUN' in morph.parse(x[i])[0].tag and 'NOUN' not in morph.parse(x[i + 1])[0].tag:\n",
    "\n",
    "            k = find_adj(x, i, pr)\n",
    "            pr = i\n",
    "\n",
    "            if ' '.join(x[k:i + 1]) in origin:\n",
    "                candidate.append((' '.join(x[k:i + 1])))\n",
    "            else:\n",
    "                begin = origin.find(x[k], len(' '.join(x[:k])))\n",
    "                end = origin.find(x[i], len(' '.join(x[:i]))) + len(x[i])\n",
    "                candidate += origin[begin:end].split(',')\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence],text]\n",
    "result = []\n",
    "file='test_sample_kws.csv'\n",
    "k=0\n",
    "\n",
    "with open(file,'r', encoding='cp1251',newline='') as f:\n",
    "    reader = csv.reader(f,delimiter=';')\n",
    "    for row in reader:\n",
    "        if row!=[]:\n",
    "            print(k)\n",
    "            k+=1\n",
    "            kws_true =''.join(row[0])\n",
    "            ann = ''.join(row[1])\n",
    "            text = ''.join(row[2])\n",
    "            if text!='':\n",
    "                kws_pred = keyword_extraction(text)\n",
    "                result.append([kws_true,kws_pred])\n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision    0.3144758735440932\n",
      "recall       0.4643734643734644\n",
      "f_measure    0.375\n"
     ]
    }
   ],
   "source": [
    "len_true = []\n",
    "len_pred = []\n",
    "len_intersection = []\n",
    "\n",
    "for i in range(len(result)-1):\n",
    "    kws_true = set(treatment_text(result[i][0])) \n",
    "    kws_pred = set(treatment_text(result[i][1])) \n",
    "    len_true.append(len(kws_true))\n",
    "    len_pred.append(len(kws_pred))\n",
    "    len_intersection.append(len(kws_true.intersection(kws_pred)))\n",
    "#     print(result[i][0])\n",
    "#     print(result[i][1])\n",
    "#     print(len(kws_true.intersection(kws_pred)))\n",
    "\n",
    "\n",
    "recall =sum(len_intersection)/sum(len_true)\n",
    "precision = sum(len_intersection)/sum(len_pred)\n",
    "print(f'precision    {precision}')\n",
    "print(f'recall       {recall}')\n",
    "print(f'f_measure    {2*precision*recall/(precision+recall)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "\n",
    "files=['./train_log/articles2019.csv','./train_log/articles2018.csv','./train_log/articles2017.csv']\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:     \n",
    "            if row!=[]:\n",
    "                kws=''.join(row[0])\n",
    "                text = ''.join(row[2])\n",
    "                sentences_origin = text.split('.')\n",
    "                sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "                kws_tr = treatment_text(kws)  \n",
    "                kws_tr_set=set(kws_tr)        \n",
    "                for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                    sentence_tr_set=set(sentence_tr)\n",
    "                    if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                    elif len(sentence_tr)>5:\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  616, 10771,   678, ...,     0,     0,     0],\n",
       "       [ 1129,  8619, 12440, ...,     0,     0,     0],\n",
       "       [ 1358,  6418,  9357, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [10798, 10957, 13584, ...,     0,     0,     0],\n",
       "       [ 1358, 10011, 11226, ...,     0,     0,     0],\n",
       "       [ 7213,  8146,  9532, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Lambda, Permute, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(fpath, label):\n",
    "#     data = []\n",
    "#     with codecs.open(fpath, 'r', 'utf-8', errors='ignore') as f:\n",
    "#         lines = f.readlines()\n",
    "#         for l in lines:\n",
    "#             l = l.rstrip()\n",
    "#             data.append((l.split(' '), label))\n",
    "#     return data\n",
    "# pos = load_data('./dataset/rt-polaritydata/rt-polarity.pos', 1)\n",
    "# neg = load_data('./dataset/rt-polaritydata/rt-polarity.neg', 0)\n",
    "# data = pos + neg\n",
    "\n",
    "x = [dataset[i][1].split() for i in range(len(dataset))]\n",
    "y = [dataset[i][2] for i in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence maxlen 407\n"
     ]
    }
   ],
   "source": [
    "sentence_maxlen = max(map(len, (d for d in x)))\n",
    "print('sentence maxlen', sentence_maxlen)\n",
    "sentence_maxlen=20\n",
    "x = [i[:sentence_maxlen] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "vocab examples: ['0', '00', '000', '0000000225676336', '0000000234101139', '00001', '0000зуx', '00017', '000244', '00034']\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "k=0\n",
    "for d in x:\n",
    "    k+=1\n",
    "    if k % 1000==0 :\n",
    "        print(k)\n",
    "    for w in d:\n",
    "        if w not in vocab: vocab.append(w)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab examples:', vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12668"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 14097\n"
     ]
    }
   ],
   "source": [
    "print('vocab size', len(vocab))\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "# i2w = {i:w for i,w in enumerate(vocab)}\n",
    "# w2i['character']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, sentence_maxlen, w2i):\n",
    "    vec_data = []\n",
    "    for d in data:\n",
    "        vec = [w2i[w] for w in d if w in w2i]\n",
    "        pad_len = max(0, sentence_maxlen - len(vec))\n",
    "        vec += [0] * pad_len\n",
    "        vec_data.append(vec)\n",
    "    vec_data = np.array(vec_data)\n",
    "    \n",
    "    return vec_data\n",
    "# random.shuffle(data)\n",
    "\n",
    "vecX = vectorize(x, sentence_maxlen, w2i)\n",
    "vecY=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(vecX)\n",
    "split_ind = (int)(n_data * 0.9)\n",
    "trainX, trainY = vecX[:split_ind], vecY[:split_ind]\n",
    "testX, testY = vecX[split_ind:], vecY[split_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n",
      "embed_matrix.shape (14097, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_glove_weights(glove_dir, embd_dim, vocab_size, word_index):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_dir, 'glove.6B.' + str(embd_dim) + 'd.txt'),encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index)) \n",
    "    embedding_matrix = np.zeros((vocab_size, embd_dim))\n",
    "    print('embed_matrix.shape', embedding_matrix.shape)\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "embd_dim = 100\n",
    "glove_embd_w = load_glove_weights(r'C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\\\', embd_dim, vocab_size, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deeppavlov.models.embedders.glove_embedder import GloVeEmbedder\n",
    "# from deeppavlov.core.data.utils import simple_download\n",
    "# # simple_download(url=\"http://files.deeppavlov.ai/embeddings/glove.6B.100d.txt\", \n",
    "# #                 destination=\"./glove.6B.100d.txt\")\n",
    "# embedder = GloVeEmbedder(load_path=r'C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\glove.6B.100d.txt',\n",
    "#                          dim=100, pad_zero=True)\n",
    "# # output shape is (batch_size x max_num_tokens_in_the_batch x embedding_dim)\n",
    "# # embedded_batch = embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "# # len(embedded_batch), len(embedded_batch[0]), embedded_batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "SentenceInput (InputLayer)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "shared_embd (Embedding)      (None, 20, 100)           1409700   \n",
      "_________________________________________________________________\n",
      "permute_12 (Permute)         (None, 100, 20)           0         \n",
      "_________________________________________________________________\n",
      "lambda_34 (Lambda)           (None, 100, 20, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 98, 1, 1)          61        \n",
      "_________________________________________________________________\n",
      "lambda_35 (Lambda)           (None, 98, 1)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 32, 1)             0         \n",
      "_________________________________________________________________\n",
      "lambda_36 (Lambda)           (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,409,794\n",
      "Trainable params: 94\n",
      "Non-trainable params: 1,409,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def Net(vocab_size, embd_size, sentence_maxlen, glove_embd_w):\n",
    "    sentence = Input((sentence_maxlen,), name='SentenceInput')\n",
    "    \n",
    "    # embedding\n",
    "    embd_layer = Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embd_size, \n",
    "                           weights=[glove_embd_w], \n",
    "                           trainable=False,\n",
    "                           name='shared_embd')\n",
    "    embd_sentence = embd_layer(sentence)\n",
    "    embd_sentence = Permute((2,1))(embd_sentence)\n",
    "    embd_sentence = Lambda(lambda x: K.expand_dims(x, -1))(embd_sentence)\n",
    "    \n",
    "    # cnn\n",
    "    cnn = Conv2D(1, \n",
    "                 kernel_size=(3, sentence_maxlen),\n",
    "                 activation='relu')(embd_sentence)\n",
    "    cnn =  Lambda(lambda x: K.sum(x, axis=3))(cnn)\n",
    "    cnn = MaxPooling1D(3)(cnn)\n",
    "    cnn = Lambda(lambda x: K.sum(x, axis=2))(cnn)\n",
    "    out = Dense(1, activation='sigmoid')(cnn)\n",
    "\n",
    "    model = Model(inputs=sentence, outputs=out, name='sentence_claccification')\n",
    "    model.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "model = Net(vocab_size, embd_dim, sentence_maxlen, glove_embd_w)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11401 samples, validate on 1267 samples\n",
      "Epoch 1/10\n",
      "11401/11401 [==============================] - ETA: 10:44 - loss: 0.8513 - acc: 0.34 - ETA: 2:45 - loss: 0.7369 - acc: 0.4688 - ETA: 1:52 - loss: 0.7184 - acc: 0.505 - ETA: 1:26 - loss: 0.7077 - acc: 0.515 - ETA: 1:10 - loss: 0.6995 - acc: 0.534 - ETA: 1:00 - loss: 0.6945 - acc: 0.528 - ETA: 53s - loss: 0.6915 - acc: 0.522 - ETA: 47s - loss: 0.6897 - acc: 0.52 - ETA: 40s - loss: 0.6835 - acc: 0.53 - ETA: 37s - loss: 0.6809 - acc: 0.53 - ETA: 33s - loss: 0.6823 - acc: 0.52 - ETA: 30s - loss: 0.6821 - acc: 0.52 - ETA: 28s - loss: 0.6826 - acc: 0.52 - ETA: 26s - loss: 0.6793 - acc: 0.52 - ETA: 24s - loss: 0.6786 - acc: 0.52 - ETA: 23s - loss: 0.6758 - acc: 0.53 - ETA: 21s - loss: 0.6758 - acc: 0.53 - ETA: 20s - loss: 0.6716 - acc: 0.54 - ETA: 19s - loss: 0.6732 - acc: 0.54 - ETA: 18s - loss: 0.6706 - acc: 0.54 - ETA: 18s - loss: 0.6690 - acc: 0.54 - ETA: 17s - loss: 0.6669 - acc: 0.55 - ETA: 16s - loss: 0.6668 - acc: 0.55 - ETA: 15s - loss: 0.6659 - acc: 0.55 - ETA: 15s - loss: 0.6655 - acc: 0.55 - ETA: 14s - loss: 0.6637 - acc: 0.55 - ETA: 14s - loss: 0.6629 - acc: 0.55 - ETA: 13s - loss: 0.6623 - acc: 0.56 - ETA: 13s - loss: 0.6630 - acc: 0.56 - ETA: 12s - loss: 0.6626 - acc: 0.56 - ETA: 12s - loss: 0.6612 - acc: 0.56 - ETA: 11s - loss: 0.6623 - acc: 0.56 - ETA: 11s - loss: 0.6611 - acc: 0.57 - ETA: 11s - loss: 0.6594 - acc: 0.57 - ETA: 10s - loss: 0.6585 - acc: 0.57 - ETA: 10s - loss: 0.6583 - acc: 0.57 - ETA: 9s - loss: 0.6592 - acc: 0.5722 - ETA: 9s - loss: 0.6594 - acc: 0.572 - ETA: 9s - loss: 0.6594 - acc: 0.570 - ETA: 8s - loss: 0.6592 - acc: 0.572 - ETA: 8s - loss: 0.6579 - acc: 0.574 - ETA: 8s - loss: 0.6573 - acc: 0.574 - ETA: 8s - loss: 0.6578 - acc: 0.574 - ETA: 7s - loss: 0.6584 - acc: 0.574 - ETA: 7s - loss: 0.6589 - acc: 0.573 - ETA: 7s - loss: 0.6584 - acc: 0.574 - ETA: 7s - loss: 0.6583 - acc: 0.575 - ETA: 6s - loss: 0.6580 - acc: 0.576 - ETA: 6s - loss: 0.6573 - acc: 0.578 - ETA: 6s - loss: 0.6562 - acc: 0.580 - ETA: 6s - loss: 0.6556 - acc: 0.581 - ETA: 6s - loss: 0.6558 - acc: 0.582 - ETA: 5s - loss: 0.6544 - acc: 0.585 - ETA: 5s - loss: 0.6539 - acc: 0.585 - ETA: 5s - loss: 0.6539 - acc: 0.585 - ETA: 5s - loss: 0.6542 - acc: 0.584 - ETA: 5s - loss: 0.6539 - acc: 0.584 - ETA: 4s - loss: 0.6532 - acc: 0.584 - ETA: 4s - loss: 0.6534 - acc: 0.584 - ETA: 4s - loss: 0.6528 - acc: 0.585 - ETA: 4s - loss: 0.6531 - acc: 0.585 - ETA: 4s - loss: 0.6527 - acc: 0.585 - ETA: 4s - loss: 0.6517 - acc: 0.587 - ETA: 4s - loss: 0.6518 - acc: 0.587 - ETA: 3s - loss: 0.6511 - acc: 0.588 - ETA: 3s - loss: 0.6509 - acc: 0.588 - ETA: 3s - loss: 0.6505 - acc: 0.589 - ETA: 3s - loss: 0.6502 - acc: 0.590 - ETA: 3s - loss: 0.6504 - acc: 0.590 - ETA: 3s - loss: 0.6505 - acc: 0.591 - ETA: 3s - loss: 0.6506 - acc: 0.591 - ETA: 2s - loss: 0.6506 - acc: 0.591 - ETA: 2s - loss: 0.6504 - acc: 0.592 - ETA: 2s - loss: 0.6504 - acc: 0.592 - ETA: 2s - loss: 0.6505 - acc: 0.592 - ETA: 2s - loss: 0.6504 - acc: 0.593 - ETA: 2s - loss: 0.6506 - acc: 0.593 - ETA: 2s - loss: 0.6504 - acc: 0.593 - ETA: 2s - loss: 0.6501 - acc: 0.594 - ETA: 2s - loss: 0.6502 - acc: 0.593 - ETA: 1s - loss: 0.6495 - acc: 0.594 - ETA: 1s - loss: 0.6494 - acc: 0.595 - ETA: 1s - loss: 0.6502 - acc: 0.594 - ETA: 1s - loss: 0.6497 - acc: 0.595 - ETA: 1s - loss: 0.6495 - acc: 0.596 - ETA: 1s - loss: 0.6491 - acc: 0.597 - ETA: 1s - loss: 0.6491 - acc: 0.597 - ETA: 1s - loss: 0.6489 - acc: 0.598 - ETA: 0s - loss: 0.6489 - acc: 0.598 - ETA: 0s - loss: 0.6495 - acc: 0.597 - ETA: 0s - loss: 0.6494 - acc: 0.598 - ETA: 0s - loss: 0.6492 - acc: 0.599 - ETA: 0s - loss: 0.6496 - acc: 0.599 - ETA: 0s - loss: 0.6496 - acc: 0.599 - ETA: 0s - loss: 0.6496 - acc: 0.600 - ETA: 0s - loss: 0.6496 - acc: 0.600 - ETA: 0s - loss: 0.6491 - acc: 0.601 - 7s 653us/step - loss: 0.6491 - acc: 0.6010 - val_loss: 0.6231 - val_acc: 0.6567\n",
      "Epoch 2/10\n",
      "11401/11401 [==============================] - ETA: 3s - loss: 0.6511 - acc: 0.625 - ETA: 3s - loss: 0.6410 - acc: 0.589 - ETA: 3s - loss: 0.6437 - acc: 0.601 - ETA: 3s - loss: 0.6431 - acc: 0.617 - ETA: 3s - loss: 0.6420 - acc: 0.610 - ETA: 3s - loss: 0.6399 - acc: 0.617 - ETA: 3s - loss: 0.6362 - acc: 0.620 - ETA: 3s - loss: 0.6325 - acc: 0.629 - ETA: 3s - loss: 0.6369 - acc: 0.627 - ETA: 3s - loss: 0.6390 - acc: 0.626 - ETA: 2s - loss: 0.6375 - acc: 0.630 - ETA: 2s - loss: 0.6401 - acc: 0.625 - ETA: 2s - loss: 0.6385 - acc: 0.631 - ETA: 2s - loss: 0.6373 - acc: 0.631 - ETA: 2s - loss: 0.6357 - acc: 0.632 - ETA: 2s - loss: 0.6382 - acc: 0.629 - ETA: 2s - loss: 0.6365 - acc: 0.630 - ETA: 2s - loss: 0.6374 - acc: 0.627 - ETA: 2s - loss: 0.6368 - acc: 0.628 - ETA: 2s - loss: 0.6380 - acc: 0.624 - ETA: 2s - loss: 0.6399 - acc: 0.623 - ETA: 2s - loss: 0.6407 - acc: 0.623 - ETA: 2s - loss: 0.6397 - acc: 0.625 - ETA: 2s - loss: 0.6399 - acc: 0.626 - ETA: 2s - loss: 0.6404 - acc: 0.625 - ETA: 2s - loss: 0.6402 - acc: 0.624 - ETA: 2s - loss: 0.6393 - acc: 0.623 - ETA: 1s - loss: 0.6389 - acc: 0.625 - ETA: 1s - loss: 0.6394 - acc: 0.623 - ETA: 1s - loss: 0.6390 - acc: 0.625 - ETA: 1s - loss: 0.6378 - acc: 0.627 - ETA: 1s - loss: 0.6369 - acc: 0.629 - ETA: 1s - loss: 0.6364 - acc: 0.630 - ETA: 1s - loss: 0.6359 - acc: 0.630 - ETA: 1s - loss: 0.6361 - acc: 0.629 - ETA: 1s - loss: 0.6358 - acc: 0.629 - ETA: 1s - loss: 0.6352 - acc: 0.631 - ETA: 1s - loss: 0.6346 - acc: 0.631 - ETA: 1s - loss: 0.6348 - acc: 0.630 - ETA: 1s - loss: 0.6344 - acc: 0.632 - ETA: 1s - loss: 0.6344 - acc: 0.632 - ETA: 1s - loss: 0.6350 - acc: 0.632 - ETA: 1s - loss: 0.6349 - acc: 0.632 - ETA: 1s - loss: 0.6348 - acc: 0.631 - ETA: 0s - loss: 0.6352 - acc: 0.630 - ETA: 0s - loss: 0.6345 - acc: 0.631 - ETA: 0s - loss: 0.6342 - acc: 0.632 - ETA: 0s - loss: 0.6349 - acc: 0.631 - ETA: 0s - loss: 0.6343 - acc: 0.632 - ETA: 0s - loss: 0.6337 - acc: 0.633 - ETA: 0s - loss: 0.6338 - acc: 0.632 - ETA: 0s - loss: 0.6337 - acc: 0.633 - ETA: 0s - loss: 0.6339 - acc: 0.632 - ETA: 0s - loss: 0.6337 - acc: 0.632 - ETA: 0s - loss: 0.6338 - acc: 0.632 - ETA: 0s - loss: 0.6337 - acc: 0.632 - ETA: 0s - loss: 0.6333 - acc: 0.633 - ETA: 0s - loss: 0.6327 - acc: 0.633 - ETA: 0s - loss: 0.6327 - acc: 0.634 - 3s 286us/step - loss: 0.6323 - acc: 0.6338 - val_loss: 0.6141 - val_acc: 0.6748\n",
      "Epoch 3/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.7384 - acc: 0.375 - ETA: 2s - loss: 0.6254 - acc: 0.614 - ETA: 2s - loss: 0.6081 - acc: 0.639 - ETA: 2s - loss: 0.6213 - acc: 0.625 - ETA: 2s - loss: 0.6183 - acc: 0.639 - ETA: 2s - loss: 0.6236 - acc: 0.629 - ETA: 2s - loss: 0.6282 - acc: 0.621 - ETA: 2s - loss: 0.6314 - acc: 0.622 - ETA: 2s - loss: 0.6319 - acc: 0.620 - ETA: 2s - loss: 0.6281 - acc: 0.625 - ETA: 1s - loss: 0.6287 - acc: 0.627 - ETA: 1s - loss: 0.6287 - acc: 0.626 - ETA: 1s - loss: 0.6257 - acc: 0.630 - ETA: 1s - loss: 0.6264 - acc: 0.629 - ETA: 1s - loss: 0.6262 - acc: 0.631 - ETA: 1s - loss: 0.6245 - acc: 0.633 - ETA: 1s - loss: 0.6243 - acc: 0.633 - ETA: 1s - loss: 0.6251 - acc: 0.634 - ETA: 1s - loss: 0.6227 - acc: 0.638 - ETA: 1s - loss: 0.6238 - acc: 0.636 - ETA: 1s - loss: 0.6254 - acc: 0.635 - ETA: 1s - loss: 0.6251 - acc: 0.634 - ETA: 1s - loss: 0.6240 - acc: 0.636 - ETA: 1s - loss: 0.6232 - acc: 0.638 - ETA: 1s - loss: 0.6231 - acc: 0.640 - ETA: 1s - loss: 0.6229 - acc: 0.640 - ETA: 1s - loss: 0.6222 - acc: 0.642 - ETA: 1s - loss: 0.6227 - acc: 0.641 - ETA: 1s - loss: 0.6223 - acc: 0.642 - ETA: 0s - loss: 0.6226 - acc: 0.642 - ETA: 0s - loss: 0.6231 - acc: 0.640 - ETA: 0s - loss: 0.6239 - acc: 0.638 - ETA: 0s - loss: 0.6243 - acc: 0.637 - ETA: 0s - loss: 0.6247 - acc: 0.638 - ETA: 0s - loss: 0.6249 - acc: 0.639 - ETA: 0s - loss: 0.6247 - acc: 0.640 - ETA: 0s - loss: 0.6243 - acc: 0.640 - ETA: 0s - loss: 0.6247 - acc: 0.640 - ETA: 0s - loss: 0.6246 - acc: 0.640 - ETA: 0s - loss: 0.6252 - acc: 0.640 - ETA: 0s - loss: 0.6253 - acc: 0.639 - ETA: 0s - loss: 0.6256 - acc: 0.638 - ETA: 0s - loss: 0.6260 - acc: 0.637 - ETA: 0s - loss: 0.6259 - acc: 0.637 - ETA: 0s - loss: 0.6253 - acc: 0.638 - ETA: 0s - loss: 0.6257 - acc: 0.638 - ETA: 0s - loss: 0.6263 - acc: 0.637 - ETA: 0s - loss: 0.6260 - acc: 0.638 - 3s 226us/step - loss: 0.6259 - acc: 0.6380 - val_loss: 0.6073 - val_acc: 0.6938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.5566 - acc: 0.750 - ETA: 2s - loss: 0.6053 - acc: 0.673 - ETA: 2s - loss: 0.6215 - acc: 0.674 - ETA: 2s - loss: 0.6247 - acc: 0.661 - ETA: 2s - loss: 0.6241 - acc: 0.655 - ETA: 2s - loss: 0.6204 - acc: 0.665 - ETA: 2s - loss: 0.6242 - acc: 0.660 - ETA: 2s - loss: 0.6222 - acc: 0.656 - ETA: 2s - loss: 0.6221 - acc: 0.653 - ETA: 2s - loss: 0.6267 - acc: 0.649 - ETA: 2s - loss: 0.6252 - acc: 0.651 - ETA: 1s - loss: 0.6290 - acc: 0.646 - ETA: 1s - loss: 0.6302 - acc: 0.644 - ETA: 1s - loss: 0.6285 - acc: 0.643 - ETA: 1s - loss: 0.6287 - acc: 0.641 - ETA: 1s - loss: 0.6282 - acc: 0.641 - ETA: 1s - loss: 0.6290 - acc: 0.640 - ETA: 1s - loss: 0.6277 - acc: 0.639 - ETA: 1s - loss: 0.6267 - acc: 0.638 - ETA: 1s - loss: 0.6265 - acc: 0.639 - ETA: 1s - loss: 0.6269 - acc: 0.639 - ETA: 1s - loss: 0.6262 - acc: 0.640 - ETA: 1s - loss: 0.6249 - acc: 0.644 - ETA: 1s - loss: 0.6243 - acc: 0.645 - ETA: 1s - loss: 0.6239 - acc: 0.647 - ETA: 1s - loss: 0.6242 - acc: 0.646 - ETA: 1s - loss: 0.6246 - acc: 0.644 - ETA: 1s - loss: 0.6235 - acc: 0.645 - ETA: 1s - loss: 0.6240 - acc: 0.643 - ETA: 0s - loss: 0.6226 - acc: 0.645 - ETA: 0s - loss: 0.6231 - acc: 0.645 - ETA: 0s - loss: 0.6224 - acc: 0.645 - ETA: 0s - loss: 0.6219 - acc: 0.647 - ETA: 0s - loss: 0.6213 - acc: 0.646 - ETA: 0s - loss: 0.6208 - acc: 0.646 - ETA: 0s - loss: 0.6202 - acc: 0.647 - ETA: 0s - loss: 0.6197 - acc: 0.648 - ETA: 0s - loss: 0.6203 - acc: 0.646 - ETA: 0s - loss: 0.6208 - acc: 0.645 - ETA: 0s - loss: 0.6209 - acc: 0.645 - ETA: 0s - loss: 0.6203 - acc: 0.645 - ETA: 0s - loss: 0.6214 - acc: 0.643 - ETA: 0s - loss: 0.6207 - acc: 0.644 - ETA: 0s - loss: 0.6206 - acc: 0.644 - ETA: 0s - loss: 0.6207 - acc: 0.644 - ETA: 0s - loss: 0.6207 - acc: 0.644 - ETA: 0s - loss: 0.6213 - acc: 0.643 - ETA: 0s - loss: 0.6216 - acc: 0.643 - ETA: 0s - loss: 0.6218 - acc: 0.642 - 3s 233us/step - loss: 0.6219 - acc: 0.6426 - val_loss: 0.6040 - val_acc: 0.6922\n",
      "Epoch 5/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.7398 - acc: 0.468 - ETA: 2s - loss: 0.6298 - acc: 0.621 - ETA: 2s - loss: 0.6238 - acc: 0.639 - ETA: 2s - loss: 0.6136 - acc: 0.636 - ETA: 2s - loss: 0.6162 - acc: 0.632 - ETA: 2s - loss: 0.6218 - acc: 0.626 - ETA: 2s - loss: 0.6240 - acc: 0.634 - ETA: 1s - loss: 0.6241 - acc: 0.636 - ETA: 1s - loss: 0.6246 - acc: 0.634 - ETA: 1s - loss: 0.6245 - acc: 0.634 - ETA: 1s - loss: 0.6218 - acc: 0.638 - ETA: 1s - loss: 0.6218 - acc: 0.641 - ETA: 1s - loss: 0.6194 - acc: 0.645 - ETA: 1s - loss: 0.6184 - acc: 0.647 - ETA: 1s - loss: 0.6184 - acc: 0.646 - ETA: 1s - loss: 0.6191 - acc: 0.647 - ETA: 1s - loss: 0.6189 - acc: 0.647 - ETA: 1s - loss: 0.6172 - acc: 0.651 - ETA: 1s - loss: 0.6161 - acc: 0.652 - ETA: 1s - loss: 0.6180 - acc: 0.647 - ETA: 1s - loss: 0.6178 - acc: 0.647 - ETA: 1s - loss: 0.6182 - acc: 0.647 - ETA: 1s - loss: 0.6189 - acc: 0.648 - ETA: 1s - loss: 0.6178 - acc: 0.650 - ETA: 1s - loss: 0.6171 - acc: 0.649 - ETA: 0s - loss: 0.6167 - acc: 0.649 - ETA: 0s - loss: 0.6173 - acc: 0.649 - ETA: 0s - loss: 0.6170 - acc: 0.649 - ETA: 0s - loss: 0.6169 - acc: 0.649 - ETA: 0s - loss: 0.6170 - acc: 0.649 - ETA: 0s - loss: 0.6182 - acc: 0.647 - ETA: 0s - loss: 0.6190 - acc: 0.645 - ETA: 0s - loss: 0.6190 - acc: 0.645 - ETA: 0s - loss: 0.6190 - acc: 0.644 - ETA: 0s - loss: 0.6196 - acc: 0.642 - ETA: 0s - loss: 0.6193 - acc: 0.643 - ETA: 0s - loss: 0.6190 - acc: 0.643 - ETA: 0s - loss: 0.6194 - acc: 0.643 - ETA: 0s - loss: 0.6196 - acc: 0.644 - ETA: 0s - loss: 0.6194 - acc: 0.644 - ETA: 0s - loss: 0.6194 - acc: 0.644 - ETA: 0s - loss: 0.6193 - acc: 0.643 - 2s 196us/step - loss: 0.6191 - acc: 0.6443 - val_loss: 0.6005 - val_acc: 0.7009\n",
      "Epoch 6/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.5588 - acc: 0.718 - ETA: 1s - loss: 0.6117 - acc: 0.644 - ETA: 1s - loss: 0.6145 - acc: 0.642 - ETA: 1s - loss: 0.6230 - acc: 0.632 - ETA: 1s - loss: 0.6190 - acc: 0.639 - ETA: 1s - loss: 0.6182 - acc: 0.638 - ETA: 1s - loss: 0.6194 - acc: 0.636 - ETA: 1s - loss: 0.6194 - acc: 0.639 - ETA: 1s - loss: 0.6168 - acc: 0.641 - ETA: 1s - loss: 0.6137 - acc: 0.647 - ETA: 1s - loss: 0.6133 - acc: 0.646 - ETA: 1s - loss: 0.6121 - acc: 0.648 - ETA: 1s - loss: 0.6132 - acc: 0.648 - ETA: 1s - loss: 0.6108 - acc: 0.651 - ETA: 1s - loss: 0.6099 - acc: 0.652 - ETA: 1s - loss: 0.6080 - acc: 0.655 - ETA: 1s - loss: 0.6096 - acc: 0.651 - ETA: 1s - loss: 0.6102 - acc: 0.650 - ETA: 1s - loss: 0.6104 - acc: 0.650 - ETA: 1s - loss: 0.6106 - acc: 0.649 - ETA: 1s - loss: 0.6103 - acc: 0.650 - ETA: 1s - loss: 0.6100 - acc: 0.651 - ETA: 1s - loss: 0.6115 - acc: 0.650 - ETA: 1s - loss: 0.6120 - acc: 0.649 - ETA: 0s - loss: 0.6119 - acc: 0.649 - ETA: 0s - loss: 0.6123 - acc: 0.649 - ETA: 0s - loss: 0.6132 - acc: 0.649 - ETA: 0s - loss: 0.6145 - acc: 0.649 - ETA: 0s - loss: 0.6144 - acc: 0.648 - ETA: 0s - loss: 0.6140 - acc: 0.648 - ETA: 0s - loss: 0.6138 - acc: 0.649 - ETA: 0s - loss: 0.6140 - acc: 0.649 - ETA: 0s - loss: 0.6134 - acc: 0.650 - ETA: 0s - loss: 0.6138 - acc: 0.650 - ETA: 0s - loss: 0.6146 - acc: 0.649 - ETA: 0s - loss: 0.6149 - acc: 0.649 - ETA: 0s - loss: 0.6153 - acc: 0.649 - ETA: 0s - loss: 0.6150 - acc: 0.650 - ETA: 0s - loss: 0.6155 - acc: 0.649 - ETA: 0s - loss: 0.6151 - acc: 0.649 - ETA: 0s - loss: 0.6150 - acc: 0.648 - ETA: 0s - loss: 0.6151 - acc: 0.647 - ETA: 0s - loss: 0.6167 - acc: 0.646 - 2s 202us/step - loss: 0.6169 - acc: 0.6463 - val_loss: 0.5982 - val_acc: 0.7096\n",
      "Epoch 7/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.6647 - acc: 0.531 - ETA: 2s - loss: 0.6072 - acc: 0.628 - ETA: 2s - loss: 0.6161 - acc: 0.628 - ETA: 2s - loss: 0.6004 - acc: 0.657 - ETA: 2s - loss: 0.5985 - acc: 0.664 - ETA: 1s - loss: 0.6040 - acc: 0.657 - ETA: 1s - loss: 0.6072 - acc: 0.655 - ETA: 1s - loss: 0.6100 - acc: 0.656 - ETA: 1s - loss: 0.6108 - acc: 0.653 - ETA: 1s - loss: 0.6093 - acc: 0.652 - ETA: 1s - loss: 0.6093 - acc: 0.649 - ETA: 1s - loss: 0.6108 - acc: 0.647 - ETA: 1s - loss: 0.6126 - acc: 0.643 - ETA: 1s - loss: 0.6122 - acc: 0.644 - ETA: 1s - loss: 0.6141 - acc: 0.641 - ETA: 1s - loss: 0.6135 - acc: 0.642 - ETA: 1s - loss: 0.6144 - acc: 0.643 - ETA: 1s - loss: 0.6152 - acc: 0.640 - ETA: 1s - loss: 0.6152 - acc: 0.641 - ETA: 1s - loss: 0.6165 - acc: 0.640 - ETA: 1s - loss: 0.6157 - acc: 0.640 - ETA: 1s - loss: 0.6141 - acc: 0.641 - ETA: 1s - loss: 0.6149 - acc: 0.641 - ETA: 0s - loss: 0.6153 - acc: 0.641 - ETA: 0s - loss: 0.6143 - acc: 0.642 - ETA: 0s - loss: 0.6141 - acc: 0.643 - ETA: 0s - loss: 0.6134 - acc: 0.643 - ETA: 0s - loss: 0.6155 - acc: 0.642 - ETA: 0s - loss: 0.6147 - acc: 0.644 - ETA: 0s - loss: 0.6139 - acc: 0.645 - ETA: 0s - loss: 0.6145 - acc: 0.644 - ETA: 0s - loss: 0.6148 - acc: 0.644 - ETA: 0s - loss: 0.6156 - acc: 0.643 - ETA: 0s - loss: 0.6148 - acc: 0.644 - ETA: 0s - loss: 0.6148 - acc: 0.644 - ETA: 0s - loss: 0.6149 - acc: 0.644 - ETA: 0s - loss: 0.6145 - acc: 0.646 - ETA: 0s - loss: 0.6145 - acc: 0.645 - ETA: 0s - loss: 0.6151 - acc: 0.645 - ETA: 0s - loss: 0.6149 - acc: 0.645 - ETA: 0s - loss: 0.6141 - acc: 0.646 - ETA: 0s - loss: 0.6148 - acc: 0.645 - 2s 199us/step - loss: 0.6152 - acc: 0.6455 - val_loss: 0.5965 - val_acc: 0.7119\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11401/11401 [==============================] - ETA: 2s - loss: 0.6820 - acc: 0.593 - ETA: 2s - loss: 0.6237 - acc: 0.656 - ETA: 2s - loss: 0.6236 - acc: 0.651 - ETA: 2s - loss: 0.6149 - acc: 0.659 - ETA: 1s - loss: 0.6168 - acc: 0.647 - ETA: 1s - loss: 0.6176 - acc: 0.646 - ETA: 1s - loss: 0.6124 - acc: 0.662 - ETA: 1s - loss: 0.6133 - acc: 0.659 - ETA: 1s - loss: 0.6135 - acc: 0.657 - ETA: 1s - loss: 0.6145 - acc: 0.654 - ETA: 1s - loss: 0.6129 - acc: 0.659 - ETA: 1s - loss: 0.6109 - acc: 0.659 - ETA: 1s - loss: 0.6105 - acc: 0.658 - ETA: 1s - loss: 0.6098 - acc: 0.659 - ETA: 1s - loss: 0.6104 - acc: 0.657 - ETA: 1s - loss: 0.6115 - acc: 0.656 - ETA: 1s - loss: 0.6117 - acc: 0.655 - ETA: 1s - loss: 0.6117 - acc: 0.654 - ETA: 1s - loss: 0.6112 - acc: 0.655 - ETA: 1s - loss: 0.6127 - acc: 0.654 - ETA: 1s - loss: 0.6134 - acc: 0.651 - ETA: 1s - loss: 0.6137 - acc: 0.651 - ETA: 1s - loss: 0.6143 - acc: 0.651 - ETA: 1s - loss: 0.6146 - acc: 0.651 - ETA: 0s - loss: 0.6144 - acc: 0.651 - ETA: 0s - loss: 0.6138 - acc: 0.651 - ETA: 0s - loss: 0.6123 - acc: 0.652 - ETA: 0s - loss: 0.6118 - acc: 0.652 - ETA: 0s - loss: 0.6123 - acc: 0.651 - ETA: 0s - loss: 0.6124 - acc: 0.651 - ETA: 0s - loss: 0.6124 - acc: 0.650 - ETA: 0s - loss: 0.6126 - acc: 0.650 - ETA: 0s - loss: 0.6130 - acc: 0.649 - ETA: 0s - loss: 0.6133 - acc: 0.648 - ETA: 0s - loss: 0.6129 - acc: 0.648 - ETA: 0s - loss: 0.6133 - acc: 0.648 - ETA: 0s - loss: 0.6127 - acc: 0.648 - ETA: 0s - loss: 0.6126 - acc: 0.647 - ETA: 0s - loss: 0.6127 - acc: 0.648 - ETA: 0s - loss: 0.6132 - acc: 0.647 - ETA: 0s - loss: 0.6133 - acc: 0.647 - ETA: 0s - loss: 0.6134 - acc: 0.646 - ETA: 0s - loss: 0.6138 - acc: 0.646 - 2s 202us/step - loss: 0.6138 - acc: 0.6464 - val_loss: 0.5952 - val_acc: 0.7096\n",
      "Epoch 9/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.5647 - acc: 0.718 - ETA: 2s - loss: 0.6112 - acc: 0.678 - ETA: 2s - loss: 0.6083 - acc: 0.668 - ETA: 2s - loss: 0.6062 - acc: 0.677 - ETA: 2s - loss: 0.6028 - acc: 0.668 - ETA: 1s - loss: 0.6047 - acc: 0.662 - ETA: 1s - loss: 0.6113 - acc: 0.659 - ETA: 1s - loss: 0.6142 - acc: 0.658 - ETA: 1s - loss: 0.6135 - acc: 0.659 - ETA: 1s - loss: 0.6144 - acc: 0.653 - ETA: 1s - loss: 0.6162 - acc: 0.648 - ETA: 1s - loss: 0.6184 - acc: 0.647 - ETA: 1s - loss: 0.6175 - acc: 0.647 - ETA: 1s - loss: 0.6162 - acc: 0.647 - ETA: 1s - loss: 0.6151 - acc: 0.648 - ETA: 1s - loss: 0.6146 - acc: 0.649 - ETA: 1s - loss: 0.6152 - acc: 0.646 - ETA: 1s - loss: 0.6150 - acc: 0.647 - ETA: 1s - loss: 0.6163 - acc: 0.644 - ETA: 1s - loss: 0.6147 - acc: 0.646 - ETA: 1s - loss: 0.6150 - acc: 0.645 - ETA: 1s - loss: 0.6166 - acc: 0.642 - ETA: 1s - loss: 0.6162 - acc: 0.643 - ETA: 0s - loss: 0.6177 - acc: 0.643 - ETA: 0s - loss: 0.6175 - acc: 0.642 - ETA: 0s - loss: 0.6171 - acc: 0.642 - ETA: 0s - loss: 0.6172 - acc: 0.643 - ETA: 0s - loss: 0.6166 - acc: 0.643 - ETA: 0s - loss: 0.6156 - acc: 0.643 - ETA: 0s - loss: 0.6158 - acc: 0.642 - ETA: 0s - loss: 0.6151 - acc: 0.642 - ETA: 0s - loss: 0.6149 - acc: 0.642 - ETA: 0s - loss: 0.6151 - acc: 0.642 - ETA: 0s - loss: 0.6149 - acc: 0.643 - ETA: 0s - loss: 0.6147 - acc: 0.644 - ETA: 0s - loss: 0.6149 - acc: 0.645 - ETA: 0s - loss: 0.6145 - acc: 0.645 - ETA: 0s - loss: 0.6135 - acc: 0.647 - ETA: 0s - loss: 0.6133 - acc: 0.647 - ETA: 0s - loss: 0.6132 - acc: 0.647 - ETA: 0s - loss: 0.6122 - acc: 0.648 - ETA: 0s - loss: 0.6125 - acc: 0.648 - ETA: 0s - loss: 0.6126 - acc: 0.647 - 2s 201us/step - loss: 0.6126 - acc: 0.6478 - val_loss: 0.5941 - val_acc: 0.7064\n",
      "Epoch 10/10\n",
      "11401/11401 [==============================] - ETA: 2s - loss: 0.6129 - acc: 0.625 - ETA: 2s - loss: 0.6234 - acc: 0.643 - ETA: 2s - loss: 0.6096 - acc: 0.639 - ETA: 2s - loss: 0.6117 - acc: 0.637 - ETA: 1s - loss: 0.6135 - acc: 0.640 - ETA: 1s - loss: 0.6204 - acc: 0.627 - ETA: 1s - loss: 0.6146 - acc: 0.638 - ETA: 1s - loss: 0.6140 - acc: 0.641 - ETA: 1s - loss: 0.6133 - acc: 0.643 - ETA: 1s - loss: 0.6121 - acc: 0.644 - ETA: 1s - loss: 0.6130 - acc: 0.646 - ETA: 1s - loss: 0.6162 - acc: 0.642 - ETA: 1s - loss: 0.6168 - acc: 0.643 - ETA: 1s - loss: 0.6181 - acc: 0.639 - ETA: 1s - loss: 0.6170 - acc: 0.639 - ETA: 1s - loss: 0.6180 - acc: 0.640 - ETA: 1s - loss: 0.6159 - acc: 0.641 - ETA: 1s - loss: 0.6132 - acc: 0.643 - ETA: 1s - loss: 0.6139 - acc: 0.643 - ETA: 1s - loss: 0.6151 - acc: 0.640 - ETA: 1s - loss: 0.6127 - acc: 0.644 - ETA: 1s - loss: 0.6133 - acc: 0.644 - ETA: 1s - loss: 0.6122 - acc: 0.645 - ETA: 0s - loss: 0.6124 - acc: 0.646 - ETA: 0s - loss: 0.6130 - acc: 0.645 - ETA: 0s - loss: 0.6131 - acc: 0.646 - ETA: 0s - loss: 0.6122 - acc: 0.649 - ETA: 0s - loss: 0.6136 - acc: 0.646 - ETA: 0s - loss: 0.6133 - acc: 0.647 - ETA: 0s - loss: 0.6137 - acc: 0.648 - ETA: 0s - loss: 0.6139 - acc: 0.647 - ETA: 0s - loss: 0.6143 - acc: 0.646 - ETA: 0s - loss: 0.6141 - acc: 0.647 - ETA: 0s - loss: 0.6141 - acc: 0.646 - ETA: 0s - loss: 0.6134 - acc: 0.647 - ETA: 0s - loss: 0.6121 - acc: 0.649 - ETA: 0s - loss: 0.6116 - acc: 0.650 - ETA: 0s - loss: 0.6116 - acc: 0.650 - ETA: 0s - loss: 0.6108 - acc: 0.651 - ETA: 0s - loss: 0.6104 - acc: 0.651 - ETA: 0s - loss: 0.6114 - acc: 0.649 - ETA: 0s - loss: 0.6118 - acc: 0.648 - 2s 200us/step - loss: 0.6115 - acc: 0.6496 - val_loss: 0.5933 - val_acc: 0.7056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x889ab1fb00>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=(testX, testY)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX = np.array([np.array(trainX[i][:100]).reshape(100,) for i in range(len(trainX))])\n",
    "# testX = np.array([np.array(testX[i][:100]).reshape(100,) for i in range(len(testX))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=set()\n",
    "# for i in trainX:\n",
    "#     d.add(len(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred = []\n",
    "for i in model.predict(testX):\n",
    "    \n",
    "    if i[0]>=0.5:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "         pred.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71       614\n",
      "           1       0.74      0.66      0.70       653\n",
      "\n",
      "    accuracy                           0.71      1267\n",
      "   macro avg       0.71      0.71      0.71      1267\n",
      "weighted avg       0.71      0.71      0.71      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = list(map(lambda x: int(x[0]),testY))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "30\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "           0       0.67      0.74      0.70       614\n",
    "           1       0.73      0.66      0.69       653\n",
    "\n",
    "    accuracy                           0.70      1267\n",
    "   macro avg       0.70      0.70      0.70      1267\n",
    "weighted avg       0.70      0.70      0.70      1267\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.68      0.74      0.71       614\n",
    "           1       0.74      0.68      0.71       653\n",
    "\n",
    "    accuracy                           0.71      1267\n",
    "   macro avg       0.71      0.71      0.71      1267\n",
    "weighted avg       0.71      0.71      0.71      1267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 \n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.67      0.76      0.71       614\n",
    "           1       0.74      0.66      0.70       653\n",
    "\n",
    "    accuracy                           0.71      1267\n",
    "   macro avg       0.71      0.71      0.71      1267\n",
    "weighted avg       0.71      0.71      0.71      1267"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
