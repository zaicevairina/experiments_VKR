{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка качества RAKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake, Metric\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "import pymorphy2\n",
    "import csv\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "from natasha import NamesExtractor\n",
    "from natasha.markup import show_markup, show_json\n",
    "for i in ['результат','др','место','небходимость','рисунок','вывод','метр','фанат','осторожность','контент','таблица','схема']:\n",
    "    stops.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_names(text):\n",
    "    extractor = NamesExtractor()\n",
    "\n",
    "    matches = extractor(text)\n",
    "    facts = [_.fact.as_json for _ in matches]\n",
    "\n",
    "    names = []\n",
    "    for i in range(len(facts)):\n",
    "        if 'first' in facts[i]:\n",
    "            x = facts[i]['first'].lower()\n",
    "            names.append(x)\n",
    "        if 'middle' in facts[i]:\n",
    "            x = facts[i]['middle'].lower()\n",
    "            names.append(x)\n",
    "        if 'last' in facts[i]:\n",
    "            x = facts[i]['last'].lower()\n",
    "            names.append(x)\n",
    "    return names\n",
    "\n",
    "\n",
    "class RAKE():\n",
    "\n",
    "    def keywords_extract(self,text):\n",
    "        names = def_names(text)\n",
    "        for i in names:\n",
    "            text.replace(i, '')\n",
    "        # разделяем текст на токены и приводим к нижнему регистру\n",
    "        tokenized_text = text.lower()\n",
    "        # убираем все лишник символы\n",
    "        tokenized_text = re.sub(\"[^а-яА-Яa-zA-Z.?!]\", \" \", tokenized_text)\n",
    "\n",
    "        tokenized_text=tokenized_text.split()\n",
    "\n",
    "        # остаяляем как потенциальные КС только сущ и прил т е в список стоп слов добавляем все остальное\n",
    "        for i in tokenized_text:\n",
    "            if 'NOUN' not in morph.parse(i)[0].tag and 'ADJF' not in morph.parse(i)[0].tag:\n",
    "                stops.add(i)\n",
    "        # print(tokenized_text)\n",
    "        tokenized_text = ' '.join(tokenized_text)\n",
    "        r = Rake(ranking_metric=Metric.WORD_FREQUENCY, stopwords=stops, max_length=2)\n",
    "        # extract keywords from text\n",
    "        r.extract_keywords_from_text(tokenized_text)\n",
    "\n",
    "        for i in r.get_ranked_phrases():\n",
    "            if 'NOUN' not in morph.parse(i)[0].tag and 'ADJF' not in morph.parse(i)[0].tag:\n",
    "                del i\n",
    "\n",
    "        keywords = r.get_ranked_phrases()[:20]\n",
    "\n",
    "        words = [morph.parse(w)[0].normal_form for w in keywords]\n",
    "        result = set()\n",
    "        for i in words:\n",
    "            if 'NOUN' in morph.parse(i)[0].tag and len(i) > 1:\n",
    "                result.add(i)\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RAKE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence],text]\n",
    "result = []\n",
    "file='test_sample_kws.csv'\n",
    "k=0\n",
    "\n",
    "with open(file,'r', encoding='cp1251',newline='') as f:\n",
    "    reader = csv.reader(f,delimiter=';')\n",
    "    for row in reader:\n",
    "        if row!=[]:\n",
    "#             print(k)\n",
    "            k+=1\n",
    "            kws_true =''.join(row[0])\n",
    "            ann = ''.join(row[1])\n",
    "            text = ''.join(row[2])\n",
    "            if text!='':\n",
    "                kws_pred = r.keywords_extract(text=text)\n",
    "                result.append([kws_true,kws_pred])\n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision    0.15594855305466238\n",
      "recall       0.24744897959183673\n",
      "f_measure    0.19132149901380668\n"
     ]
    }
   ],
   "source": [
    "len_true = []\n",
    "len_pred = []\n",
    "len_intersection = []\n",
    "\n",
    "for i in range(len(result)-1):\n",
    "    kws_true = set(treatment_text(result[i][0])) \n",
    "    kws_pred = set(treatment_text(' '.join(result[i][1]))) \n",
    "    len_true.append(len(kws_true))\n",
    "    len_pred.append(len(kws_pred))\n",
    "    len_intersection.append(len(kws_true.intersection(kws_pred)))\n",
    "#     print(result[i][0])\n",
    "#     print(result[i][1])\n",
    "#     print(len(kws_true.intersection(kws_pred)))\n",
    "\n",
    "\n",
    "recall =sum(len_intersection)/sum(len_true)\n",
    "precision = sum(len_intersection)/sum(len_pred)\n",
    "print(f'precision    {precision}')\n",
    "print(f'recall       {recall}')\n",
    "print(f'f_measure    {2*precision*recall/(precision+recall)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
