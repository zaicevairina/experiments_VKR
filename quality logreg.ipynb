{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Проверка качества классификатора***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "file='articles.csv'\n",
    "\n",
    "with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    for row in reader:            \n",
    "        if row!=[]:\n",
    "            kws=''.join(row[0])\n",
    "            text = ''.join(row[2])\n",
    "            sentences_origin = text.split('.')\n",
    "            sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "            kws_tr = treatment_text(kws)  \n",
    "            kws_tr_set=set(kws_tr)  \n",
    "            for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                sentence_tr_set=set(sentence_tr)\n",
    "                if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                    dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                elif len(sentence_tr)>5:\n",
    "                    dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to C:\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "with open('cls_model.pickle', 'rb') as f:\n",
    "    cls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tf_idf_model.pickle',  'rb') as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = [dataset[i][1] for i in range(len(dataset))]\n",
    "y_true = [dataset[i][2] for i in range(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=cls(tfidf(x_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.85      0.71      2224\n",
      "           1       0.80      0.52      0.63      2558\n",
      "\n",
      "    accuracy                           0.68      4782\n",
      "   macro avg       0.71      0.69      0.67      4782\n",
      "weighted avg       0.71      0.68      0.67      4782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = list(map(lambda x: int(x[0]),y_true))\n",
    "pred = list(map(lambda x: int(x[0]),pred))\n",
    "print(classification_report(y_true, pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверка качества выделения ключевых слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from deeppavlov.metrics.accuracy import sets_accuracy\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# credentials = service_account.Credentials.from_service_account_file('./diplom-275218-a097a7baee3f.json')\n",
    "# project_id = 'diplom-275218'\n",
    "# private_key = 'diplom-275218-a097a7baee3f.json'\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "\n",
    "dict_stop=set(['метод','определение','условие','момент','значение','результат','критерий',\n",
    "               'работа','вариант','брянский государственный университет','научнотехнический вестник',\n",
    "              'соответствие','такой образ','весь критерий','пример','выбор','ключевое слово','период',\n",
    "              'уравнение','формула','множитель','повышение','оценка','проведение',\n",
    "              'машина','нагрузка','брянская область','точка','случай','расчет','таблица','расчёт',\n",
    "              'с показатель','град','обработка','статья','элемент','раз','применение','центр','форма','важная задача'])\n",
    "\n",
    "path = 'C:/Users/Ирина/Desktop/keywords'\n",
    "\n",
    "with open(path+'/cls_model.pickle', 'rb') as f:\n",
    "    cls = pickle.load(f)\n",
    "\n",
    "with open(path+'/tf_idf_model.pickle', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "def treatment_text2(text):\n",
    "    text = re.sub(\"[^а-яА-Яa-zA-Z0-9,.?]\", \" \", str(text))\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    while text.find('  ')!=-1:\n",
    "        text = text.replace('  ',' ')\n",
    "    text = str(text)\n",
    "    return text\n",
    "\n",
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-ЯЁёa-zA-Z0-9]\", \" \", review)\n",
    "    review_text = review_text.replace('ё','е')\n",
    "    review_text = review_text.replace('Ё', 'Е')\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "\n",
    "def normalize(keyphrase):\n",
    "    try:\n",
    "        words = keyphrase.split()\n",
    "        if len(words)>1:\n",
    "            main_noun = words[-1]\n",
    "            flag_noun = False\n",
    "            flag_adj = False\n",
    "            for i in words[:-1]:\n",
    "                if 'NOUN' not in  morph.parse(i)[0].tag and 'ADJF' not in morph.parse(i)[0].tag:\n",
    "                    return morph.parse(words[0])[0].lexeme[0][0] + ','+morph.parse(words[2])[0].lexeme[0][0]\n",
    "                if 'NOUN' in morph.parse(i)[0].tag:\n",
    "                    flag_noun=True\n",
    "                if 'ADJF' in morph.parse(i)[0].tag:\n",
    "                    flag_adj=True\n",
    "            s = ''\n",
    "\n",
    "            if flag_noun is False:\n",
    "                for word in words[:-1]:\n",
    "                    for i in morph.parse(word)[0].lexeme:\n",
    "                        if morph.parse(main_noun)[0].tag.gender in i.tag and 'nomn' in i.tag:\n",
    "\n",
    "                            s+=i.word + ' '\n",
    "                            break\n",
    "                return s + morph.parse(main_noun)[0].lexeme[0][0]\n",
    "\n",
    "            elif flag_noun is True and flag_adj is False:\n",
    "                s = morph.parse(words[0])[0].lexeme[0][0]\n",
    "                for i in words[1:]:\n",
    "                    try:\n",
    "                        s+= ' '+morph.parse(i)[0].lexeme[1][0]\n",
    "                    except:\n",
    "                        pass\n",
    "                return s\n",
    "\n",
    "            else:\n",
    "                flag_first_noun=False\n",
    "\n",
    "                for word in words:\n",
    "                    if 'ADJF' in  morph.parse(word)[0].tag and flag_first_noun is False:\n",
    "                        rod = find_rod(words[words.index(word)+1:])\n",
    "                        for i in morph.parse(word)[0].lexeme:\n",
    "                            if rod in i.tag and 'nomn' in i.tag:\n",
    "\n",
    "                                s+=i.word + ' '\n",
    "                                break\n",
    "                    elif 'ADJF' in  morph.parse(word)[0].tag and flag_first_noun is True:\n",
    "                        rod = find_rod(words[words.index(word)+1:])\n",
    "                        for i in morph.parse(word)[0].lexeme:\n",
    "                            if rod in i.tag and 'gent' in i.tag:\n",
    "                                s+=i.word + ' '\n",
    "                                break\n",
    "                    elif  'NOUN' in  morph.parse(word)[0].tag and flag_first_noun is False:\n",
    "                        flag_first_noun=True\n",
    "\n",
    "                        s += morph.parse(word)[0].lexeme[0][0] + ' '\n",
    "                    else:\n",
    "\n",
    "                        s += morph.parse(word)[0].lexeme[2][0] +' '\n",
    "\n",
    "\n",
    "            return s\n",
    "        elif len(words)==1:\n",
    "            return morph.parse(words[0])[0].lexeme[0][0]\n",
    "    except:\n",
    "        pass\n",
    "#         print(' '.join(words),'НЕ МОЖЕТ БЫТЬ ПРЕОБРАЗОВАН')\n",
    "\n",
    "def find_rod(words):\n",
    "    for w in words:\n",
    "        if 'NOUN' in morph.parse(w)[0].tag:\n",
    "            return morph.parse(w)[0].tag.gender\n",
    "\n",
    "\n",
    "def find_adj(sentence, index_noun, pred_noun):\n",
    "    if pred_noun == 0 and index_noun == 0:\n",
    "        return 0\n",
    "    if pred_noun == 0 and index_noun != 0:\n",
    "        i = index_noun - 1\n",
    "        flag_ad = False\n",
    "        flag_noun = False\n",
    "        while i >= 0:\n",
    "            if 'ADJF' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_ad = True\n",
    "            elif 'NOUN' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_noun = True\n",
    "            elif flag_ad is False and flag_noun is False:\n",
    "                return i + 1\n",
    "\n",
    "            flag_ad = False\n",
    "            flag_noun = False\n",
    "            i -= 1\n",
    "        return i + 1\n",
    "\n",
    "    else:\n",
    "        i = index_noun - 1\n",
    "        flag_ad = False\n",
    "        flag_noun = False\n",
    "        while i > pred_noun:\n",
    "            if 'ADJF' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_ad = True\n",
    "            elif 'NOUN' in morph.parse(sentence[i])[0].tag:\n",
    "                flag_noun = True\n",
    "            elif flag_ad is False and flag_noun is False:\n",
    "                return i + 1\n",
    "\n",
    "            flag_ad = False\n",
    "            flag_noun = False\n",
    "            i -= 1\n",
    "        return i + 1\n",
    "\n",
    "\n",
    "def keyword_extraction(text):\n",
    "    sentences_origin = text.split('.')\n",
    "    sentences_tr = list(map(treatment_text, sentences_origin))\n",
    "    test=[]\n",
    "    for sentence_tr, sentence_or in zip(sentences_tr, sentences_origin):\n",
    "        # sentence_tr_set = set(sentence_tr)\n",
    "        # if len(sentence_tr) > 5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "        #     test.append((sentence_or, ' '.join(sentence_tr)))\n",
    "        if len(sentence_tr) > 5:\n",
    "            test.append((sentence_or, ' '.join(sentence_tr)))\n",
    "    x_test = []\n",
    "    for s in test:\n",
    "        x_test.append(s[1])\n",
    "#     x_test = tuple(x_test)\n",
    "#     y_pred = model.predict(tokenizer.texts_to_matrix(x_test, mode='binary'))\n",
    "#     print(y_pred,x_test)\n",
    "\n",
    "\n",
    "\n",
    "    y_pred2 = cls(tfidf(x_test))\n",
    "    \n",
    "\n",
    "    sentences_with_kw = []\n",
    "    for i in range(len(y_pred2)):\n",
    "        if y_pred2[i] == '1':\n",
    "#             print(test[i])\n",
    "            sentences_with_kw.append(test[i])\n",
    "    dict_cand = {}\n",
    "    for sentence in sentences_with_kw:\n",
    "        cand = extract_candidate(sentence[0], sentences_with_kw)\n",
    "        cand = list(map(normalize, cand))\n",
    "        for key_cand in cand:\n",
    "            if key_cand in dict_cand:\n",
    "                dict_cand[key_cand] += 1\n",
    "            else:\n",
    "                dict_cand[key_cand] = 1\n",
    "    dict_cand_sort = {k: v for k, v in sorted(dict_cand.items(), key=lambda item: item[1], reverse=True)}\n",
    "    result = []\n",
    "    count = 0\n",
    "    k = set(['1', '2', '3', '4', '5', '&', '?', '/', '\\\\', '!'])\n",
    "\n",
    "    for word in dict_cand_sort.keys():\n",
    "        try:\n",
    "            if count < 10:\n",
    "                if ',' in word:\n",
    "                    word = word.split()\n",
    "                    result += word\n",
    "                elif k.intersection(set(word)) == set() and word not in dict_stop and 'ADJF' not in morph.parse(word)[\n",
    "                    0].tag:\n",
    "                    result.append(word)\n",
    "                    count += 1\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    for i in result:\n",
    "\n",
    "        flag = False\n",
    "        s = i.replace(',', ' ')\n",
    "        s = s.split(' ')\n",
    "        for j in s:\n",
    "            if 'NOUN' in morph.parse(j)[0].tag:\n",
    "                flag = True\n",
    "        if flag is False:\n",
    "            result.remove(i)\n",
    "    result = list(map(lambda x: x.replace(',', ' '), result))\n",
    "    result = ', '.join(result)\n",
    "    if result!='' :\n",
    "        return result\n",
    "#     else:\n",
    "#         keywords = r.keywords_extract(text)\n",
    "#         keywords = ', '.join(keywords).capitalize()\n",
    "#         return keywords\n",
    "def extract_candidate(origin, sentences_with_kw):\n",
    "    candidate = []\n",
    "    x = re.sub(\"[^а-яА-Я]\", \" \", origin)\n",
    "    while x.find('  ') != -1:\n",
    "        x = x.replace('  ', ' ')\n",
    "    x = x.split()\n",
    "    x = list(filter(lambda x: len(x) > 2, x))\n",
    "    pr = 0\n",
    "\n",
    "    for i in range(len(x) - 1):\n",
    "        if 'NOUN' in morph.parse(x[i])[0].tag and 'NOUN' in morph.parse(x[i + 1])[0].tag and i + 2 != len(x):\n",
    "            pass\n",
    "        elif 'NOUN' in morph.parse(x[i])[0].tag and 'NOUN' in morph.parse(x[i + 1])[0].tag and i + 2 == len(x):\n",
    "            try:\n",
    "                k = find_adj(x, i + 2, pr)\n",
    "                pr = i\n",
    "                if ' '.join(x[k:i + 1]) in sentences_with_kw[2][0]:\n",
    "                    candidate.append(' '.join(x[k:i + 2]))\n",
    "                else:\n",
    "                    begin = origin.find(x[k])\n",
    "                    end = origin.find(x[i + 1]) + len(x[i + 1])\n",
    "                    candidate += origin[begin:end].split(',')\n",
    "            except:\n",
    "                pass\n",
    "        elif 'NOUN' in morph.parse(x[i])[0].tag and 'NOUN' not in morph.parse(x[i + 1])[0].tag:\n",
    "\n",
    "            k = find_adj(x, i, pr)\n",
    "            pr = i\n",
    "\n",
    "            if ' '.join(x[k:i + 1]) in origin:\n",
    "                candidate.append((' '.join(x[k:i + 1])))\n",
    "            else:\n",
    "                begin = origin.find(x[k], len(' '.join(x[:k])))\n",
    "                end = origin.find(x[i], len(' '.join(x[:i]))) + len(x[i])\n",
    "                candidate += origin[begin:end].split(',')\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence],text]\n",
    "result = []\n",
    "file='test_sample_kws.csv'\n",
    "k=0\n",
    "\n",
    "with open(file,'r', encoding='cp1251',newline='') as f:\n",
    "    reader = csv.reader(f,delimiter=';')\n",
    "    for row in reader:\n",
    "        if row!=[]:\n",
    "#             print(k)\n",
    "            k+=1\n",
    "            kws_true =''.join(row[0])\n",
    "            ann = ''.join(row[1])\n",
    "            text = ''.join(row[2])\n",
    "            if text!='':\n",
    "                kws_pred = keyword_extraction(text)\n",
    "                result.append([kws_true,kws_pred])\n",
    "            \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision    0.32318104906937395\n",
      "recall       0.4692874692874693\n",
      "f_measure    0.3827655310621243\n"
     ]
    }
   ],
   "source": [
    "len_true = []\n",
    "len_pred = []\n",
    "len_intersection = []\n",
    "\n",
    "for i in range(len(result)-1):\n",
    "    kws_true = set(treatment_text(result[i][0])) \n",
    "    kws_pred = set(treatment_text(result[i][1])) \n",
    "    len_true.append(len(kws_true))\n",
    "    len_pred.append(len(kws_pred))\n",
    "    len_intersection.append(len(kws_true.intersection(kws_pred)))\n",
    "#     print(result[i][0])\n",
    "#     print(result[i][1])\n",
    "#     print(len(kws_true.intersection(kws_pred)))\n",
    "\n",
    "\n",
    "recall =sum(len_intersection)/sum(len_true)\n",
    "precision = sum(len_intersection)/sum(len_pred)\n",
    "print(f'precision    {precision}')\n",
    "print(f'recall       {recall}')\n",
    "print(f'f_measure    {2*precision*recall/(precision+recall)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "\n",
    "files=['./train_log/articles2019.csv','./train_log/articles2018.csv','./train_log/articles2017.csv']\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:     \n",
    "            if row!=[]:\n",
    "                kws=''.join(row[0])\n",
    "                text = ''.join(row[2])\n",
    "                sentences_origin = text.split('.')\n",
    "                sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "                kws_tr = treatment_text(kws)  \n",
    "                kws_tr_set=set(kws_tr)        \n",
    "                for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                    sentence_tr_set=set(sentence_tr)\n",
    "                    if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                    elif len(sentence_tr)>5:\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прочитать dataset из csv файла\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path=k,\n",
    "    train='dataset.csv',\n",
    "    x = 'text',\n",
    "    y = 'tag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2\n",
    "train_iterator = BasicClassificationDatasetIterator(\n",
    "    data=dr,\n",
    "    field_to_split='train',  # field that will be splitted\n",
    "    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n",
    "    split_proportions=[0.8, 0.2],  #proportions for splitting\n",
    "    split_seed=23,  # seed for splitting dataset 23\n",
    "    seed=42)  # seed for iteration over dataset 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "train_x_lower_tokenized = tokenizer(train_iterator.get_instances(data_type='train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vocabulary to collect all appeared in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='.classes.dict',\n",
    "    load_path='./classes.dict')\n",
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./tokens.dict',\n",
    "    load_path='./tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')\n",
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')\n",
    "tfidf.fit(train_iterator.get_instances(data_type='train')[0])\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
